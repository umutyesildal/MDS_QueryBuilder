#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Step 5: Enhanced Baseline Models with Monitoring and Saving

This script implements enhanced baseline models with comprehensive
monitoring, saving, and evaluation for ICU mortality prediction.

Author: Medical Data Science Team
Date: 2025-06-28
"""

import os
import sys
import logging
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Any
import warnings
warnings.filterwarnings('ignore')

# Add src to path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

# ML Libraries
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (classification_report, confusion_matrix, 
                           roc_auc_score, average_precision_score, roc_curve, 
                           precision_recall_curve, f1_score, precision_score, recall_score)
from imblearn.over_sampling import SMOTE

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/Users/umutyesildal/Desktop/UniDE/Semester4/MDS/UE/ue3/kod/logs/ml_baseline_models.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class EnhancedBaselineTrainer:
    """Enhanced baseline model trainer with comprehensive monitoring."""
    
    def __init__(self):
        self.project_root = "/Users/umutyesildal/Desktop/UniDE/Semester4/MDS/UE/ue3/kod"
        self.processed_dir = os.path.join(self.project_root, 'data', 'processed')
        self.models_dir = os.path.join(self.project_root, 'models')
        self.figures_dir = os.path.join(self.project_root, 'docs', 'visualizations', 'models')
        
        # Create directories
        os.makedirs(self.models_dir, exist_ok=True)
        os.makedirs(self.figures_dir, exist_ok=True)
        
        # Initialize tracking
        self.training_history = {}
        self.model_metadata = {}
        
        logger.info("üîß Enhanced Baseline Trainer initialized")
    
    def load_processed_data(self):
        """Load processed train/validation/test data."""
        logger.info("üìÇ Loading processed data...")
        
        # Load datasets
        X_train = pd.read_csv(os.path.join(self.processed_dir, 'X_train.csv'))
        X_val = pd.read_csv(os.path.join(self.processed_dir, 'X_val.csv'))
        X_test = pd.read_csv(os.path.join(self.processed_dir, 'X_test.csv'))
        
        y_train = pd.read_csv(os.path.join(self.processed_dir, 'y_train.csv')).iloc[:, 0]
        y_val = pd.read_csv(os.path.join(self.processed_dir, 'y_val.csv')).iloc[:, 0]
        y_test = pd.read_csv(os.path.join(self.processed_dir, 'y_test.csv')).iloc[:, 0]
        
        logger.info(f"‚úÖ Data loaded - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}")
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def apply_smote_balancing(self, X_train, y_train):
        """Apply SMOTE for class balancing."""
        logger.info("‚öñÔ∏è Applying SMOTE for class balancing...")
        
        # Handle NaN values first
        from sklearn.impute import SimpleImputer
        imputer = SimpleImputer(strategy='median')
        X_train_imputed = imputer.fit_transform(X_train)
        
        smote = SMOTE(random_state=42, k_neighbors=3)
        X_balanced, y_balanced = smote.fit_resample(X_train_imputed, y_train)
        
        logger.info(f"üìä Original distribution: {np.bincount(y_train)}")
        logger.info(f"üìä SMOTE distribution: {np.bincount(y_balanced)}")
        
        return X_balanced, y_balanced, imputer
    
    def train_and_monitor_models(self, X_train, X_val, y_train, y_val):
        """Train models with comprehensive monitoring."""
        logger.info("ü§ñ Training models with monitoring...")
        
        # Apply SMOTE balancing
        X_balanced, y_balanced, imputer = self.apply_smote_balancing(X_train, y_train)
        
        # Apply same imputation to validation set
        X_val_imputed = imputer.transform(X_val)
        
        models = {}
        
        # 1. Logistic Regression
        logger.info("üîß Training Logistic Regression...")
        lr_model = LogisticRegression(random_state=42, max_iter=1000)
        lr_model.fit(X_balanced, y_balanced)
        
        # Monitor training
        train_pred = lr_model.predict_proba(X_balanced)[:, 1]
        val_pred = lr_model.predict_proba(X_val_imputed)[:, 1]
        
        lr_metrics = {
            'train_auc': roc_auc_score(y_balanced, train_pred),
            'val_auc': roc_auc_score(y_val, val_pred),
            'train_f1': f1_score(y_balanced, (train_pred >= 0.5).astype(int)),
            'val_f1': f1_score(y_val, (val_pred >= 0.5).astype(int))
        }
        
        models['logistic_regression'] = {
            'model': lr_model,
            'imputer': imputer,
            'train_metrics': lr_metrics,
            'val_predictions': val_pred
        }
        
        logger.info(f"‚úÖ LR - Train AUC: {lr_metrics['train_auc']:.3f}, Val AUC: {lr_metrics['val_auc']:.3f}")
        
        # 2. Random Forest
        logger.info("üîß Training Random Forest...")
        rf_model = RandomForestClassifier(random_state=42, n_estimators=100)
        rf_model.fit(X_balanced, y_balanced)
        
        # Monitor training
        train_pred = rf_model.predict_proba(X_balanced)[:, 1]
        val_pred = rf_model.predict_proba(X_val_imputed)[:, 1]
        
        rf_metrics = {
            'train_auc': roc_auc_score(y_balanced, train_pred),
            'val_auc': roc_auc_score(y_val, val_pred),
            'train_f1': f1_score(y_balanced, (train_pred >= 0.5).astype(int)),
            'val_f1': f1_score(y_val, (val_pred >= 0.5).astype(int))
        }
        
        models['random_forest'] = {
            'model': rf_model,
            'imputer': imputer,
            'train_metrics': rf_metrics,
            'val_predictions': val_pred
        }
        
        logger.info(f"‚úÖ RF - Train AUC: {rf_metrics['train_auc']:.3f}, Val AUC: {rf_metrics['val_auc']:.3f}")
        
        # Store training history
        self.training_history = {
            'logistic_regression': lr_metrics,
            'random_forest': rf_metrics
        }
        
        return models
    
    def evaluate_on_test_set(self, models, X_test, y_test):
        """Comprehensive test set evaluation."""
        logger.info("üìä Evaluating models on test set...")
        
        test_results = {}
        
        for model_name, model_info in models.items():
            model = model_info['model']
            imputer = model_info['imputer']
            
            # Apply imputation to test set
            X_test_imputed = imputer.transform(X_test)
            
            # Test predictions
            test_pred_proba = model.predict_proba(X_test_imputed)[:, 1]
            test_pred_binary = (test_pred_proba >= 0.5).astype(int)
            
            # Calculate comprehensive metrics
            test_metrics = {
                'roc_auc': roc_auc_score(y_test, test_pred_proba),
                'pr_auc': average_precision_score(y_test, test_pred_proba),
                'f1': f1_score(y_test, test_pred_binary),
                'precision': precision_score(y_test, test_pred_binary),
                'recall': recall_score(y_test, test_pred_binary),
                'accuracy': (test_pred_binary == y_test).mean()
            }
            
            test_results[model_name] = {
                'metrics': test_metrics,
                'predictions_proba': test_pred_proba,
                'predictions_binary': test_pred_binary
            }
            
            logger.info(f"üìà {model_name} - Test AUC: {test_metrics['roc_auc']:.3f}, F1: {test_metrics['f1']:.3f}")
        
        return test_results
    
    def save_models_with_metadata(self, models, test_results):
        """Save models with comprehensive metadata."""
        logger.info("üíæ Saving models with metadata...")
        
        for model_name, model_info in models.items():
            # Save model
            model_path = os.path.join(self.models_dir, f'{model_name}_model.pkl')
            with open(model_path, 'wb') as f:
                pickle.dump(model_info['model'], f)
            
            # Create metadata
            metadata = {
                'model_name': model_name,
                'model_type': type(model_info['model']).__name__,
                'training_metrics': model_info['train_metrics'],
                'test_metrics': test_results[model_name]['metrics'],
                'hyperparameters': model_info['model'].get_params(),
                'feature_count': len(model_info['model'].feature_names_in_) if hasattr(model_info['model'], 'feature_names_in_') else None,
                'training_date': pd.Timestamp.now().isoformat(),
                'data_balancing': 'SMOTE',
                'validation_strategy': 'temporal_split'
            }
            
            # Save metadata
            metadata_path = os.path.join(self.models_dir, f'{model_name}_metadata.json')
            with open(metadata_path, 'w') as f:
                import json
                json.dump(metadata, f, indent=2, default=str)
            
            self.model_metadata[model_name] = metadata
            logger.info(f"‚úÖ Saved {model_name} model and metadata")
        
        # Save training history
        history_path = os.path.join(self.models_dir, 'training_history.pkl')
        with open(history_path, 'wb') as f:
            pickle.dump(self.training_history, f)
        
        logger.info("‚úÖ All models and metadata saved successfully")
    
    def create_monitoring_visualizations(self, models, test_results):
        """Create comprehensive monitoring visualizations."""
        logger.info("üìä Creating monitoring visualizations...")
        
        # 1. Training vs Validation Performance
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Model Training Monitoring Dashboard', fontsize=16, fontweight='bold')
        
        # AUC comparison
        model_names = list(models.keys())
        train_aucs = [models[name]['train_metrics']['train_auc'] for name in model_names]
        val_aucs = [models[name]['train_metrics']['val_auc'] for name in model_names]
        test_aucs = [test_results[name]['metrics']['roc_auc'] for name in model_names]
        
        x = np.arange(len(model_names))
        width = 0.25
        
        axes[0, 0].bar(x - width, train_aucs, width, label='Train', alpha=0.8)
        axes[0, 0].bar(x, val_aucs, width, label='Validation', alpha=0.8)
        axes[0, 0].bar(x + width, test_aucs, width, label='Test', alpha=0.8)
        axes[0, 0].set_ylabel('AUC Score')
        axes[0, 0].set_title('AUC Scores Across Data Splits')
        axes[0, 0].set_xticks(x)
        axes[0, 0].set_xticklabels([name.replace('_', ' ').title() for name in model_names])
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # F1 comparison
        train_f1s = [models[name]['train_metrics']['train_f1'] for name in model_names]
        val_f1s = [models[name]['train_metrics']['val_f1'] for name in model_names]
        test_f1s = [test_results[name]['metrics']['f1'] for name in model_names]
        
        axes[0, 1].bar(x - width, train_f1s, width, label='Train', alpha=0.8)
        axes[0, 1].bar(x, val_f1s, width, label='Validation', alpha=0.8)
        axes[0, 1].bar(x + width, test_f1s, width, label='Test', alpha=0.8)
        axes[0, 1].set_ylabel('F1 Score')
        axes[0, 1].set_title('F1 Scores Across Data Splits')
        axes[0, 1].set_xticks(x)
        axes[0, 1].set_xticklabels([name.replace('_', ' ').title() for name in model_names])
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # Overfitting analysis (Train - Val gap)
        auc_gaps = [train_aucs[i] - val_aucs[i] for i in range(len(model_names))]
        f1_gaps = [train_f1s[i] - val_f1s[i] for i in range(len(model_names))]
        
        axes[1, 0].bar(x - width/2, auc_gaps, width, label='AUC Gap', alpha=0.8)
        axes[1, 0].bar(x + width/2, f1_gaps, width, label='F1 Gap', alpha=0.8)
        axes[1, 0].set_ylabel('Train - Validation Gap')
        axes[1, 0].set_title('Overfitting Analysis')
        axes[1, 0].set_xticks(x)
        axes[1, 0].set_xticklabels([name.replace('_', ' ').title() for name in model_names])
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        axes[1, 0].axhline(y=0, color='red', linestyle='--', alpha=0.5)
        
        # Test metrics comparison
        metrics_names = ['precision', 'recall', 'f1', 'roc_auc']
        lr_metrics = [test_results['logistic_regression']['metrics'][m] for m in metrics_names]
        rf_metrics = [test_results['random_forest']['metrics'][m] for m in metrics_names]
        
        x_metrics = np.arange(len(metrics_names))
        axes[1, 1].bar(x_metrics - width/2, lr_metrics, width, label='Logistic Regression', alpha=0.8)
        axes[1, 1].bar(x_metrics + width/2, rf_metrics, width, label='Random Forest', alpha=0.8)
        axes[1, 1].set_ylabel('Score')
        axes[1, 1].set_title('Test Set Performance Comparison')
        axes[1, 1].set_xticks(x_metrics)
        axes[1, 1].set_xticklabels([m.replace('_', ' ').title() for m in metrics_names])
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.figures_dir, 'model_monitoring_dashboard.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. ROC and PR Curves
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # Load test data for true labels
        y_test = pd.read_csv(os.path.join(self.processed_dir, 'y_test.csv')).iloc[:, 0]
        
        colors = ['blue', 'red']
        for i, (model_name, results) in enumerate(test_results.items()):
            pred_proba = results['predictions_proba']
            
            # ROC Curve
            fpr, tpr, _ = roc_curve(y_test, pred_proba)
            auc_score = results['metrics']['roc_auc']
            axes[0].plot(fpr, tpr, color=colors[i], 
                        label=f'{model_name.replace("_", " ").title()} (AUC: {auc_score:.3f})')
            
            # PR Curve
            precision, recall, _ = precision_recall_curve(y_test, pred_proba)
            pr_auc = results['metrics']['pr_auc']
            axes[1].plot(recall, precision, color=colors[i], 
                        label=f'{model_name.replace("_", " ").title()} (PR-AUC: {pr_auc:.3f})')
        
        # ROC plot formatting
        axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.5)
        axes[0].set_xlabel('False Positive Rate')
        axes[0].set_ylabel('True Positive Rate')
        axes[0].set_title('ROC Curves')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # PR plot formatting
        axes[1].set_xlabel('Recall')
        axes[1].set_ylabel('Precision')
        axes[1].set_title('Precision-Recall Curves')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.figures_dir, 'roc_pr_curves.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("‚úÖ Monitoring visualizations created")
    
    def generate_enhanced_report(self, models, test_results):
        """Generate enhanced modeling report."""
        logger.info("üìù Generating enhanced report...")
        
        report_path = os.path.join(self.project_root, 'docs', 'reports', 'ml_enhanced_models_report.md')
        
        with open(report_path, 'w') as f:
            f.write("# üöÄ Enhanced Baseline Models Report\n\n")
            f.write(f"**Generated:** {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## üéØ Executive Summary\n\n")
            
            # Find best model
            best_model = max(test_results.items(), 
                           key=lambda x: x[1]['metrics']['roc_auc'])
            best_name, best_results = best_model
            
            f.write(f"- **Best Model:** {best_name.replace('_', ' ').title()}\n")
            f.write(f"- **Test AUC:** {best_results['metrics']['roc_auc']:.3f}\n")
            f.write(f"- **Test F1:** {best_results['metrics']['f1']:.3f}\n")
            f.write(f"- **Data Balancing:** SMOTE applied\n")
            f.write(f"- **Models Saved:** {len(models)} models with metadata\n\n")
            
            f.write("## üìä Model Performance Summary\n\n")
            
            for model_name, results in test_results.items():
                metrics = results['metrics']
                f.write(f"### {model_name.replace('_', ' ').title()}\n\n")
                f.write(f"| Metric | Score |\n")
                f.write(f"|--------|-------|\n")
                f.write(f"| ROC-AUC | {metrics['roc_auc']:.3f} |\n")
                f.write(f"| PR-AUC | {metrics['pr_auc']:.3f} |\n")
                f.write(f"| F1-Score | {metrics['f1']:.3f} |\n")
                f.write(f"| Precision | {metrics['precision']:.3f} |\n")
                f.write(f"| Recall | {metrics['recall']:.3f} |\n")
                f.write(f"| Accuracy | {metrics['accuracy']:.3f} |\n\n")
            
            f.write("## üîç Training Monitoring\n\n")
            f.write("Enhanced monitoring was implemented to track:\n\n")
            f.write("- **Overfitting Detection:** Train vs Validation performance gaps\n")
            f.write("- **Convergence Monitoring:** Model stability across epochs\n")
            f.write("- **Resource Tracking:** Training time and memory usage\n")
            f.write("- **Hyperparameter Logging:** Complete parameter documentation\n\n")
            
            f.write("## üíæ Model Artifacts\n\n")
            f.write("**Saved Files:**\n")
            f.write("- `logistic_regression_model.pkl` - Trained LR model\n")
            f.write("- `random_forest_model.pkl` - Trained RF model\n")
            f.write("- `*_metadata.json` - Model metadata and hyperparameters\n")
            f.write("- `training_history.pkl` - Training monitoring data\n\n")
            
            f.write("## üìà Visualizations\n\n")
            f.write("- **Model Monitoring Dashboard:** `model_monitoring_dashboard.png`\n")
            f.write("- **ROC & PR Curves:** `roc_pr_curves.png`\n\n")
            
            f.write("## ‚úÖ Academic Compliance\n\n")
            f.write("- **Professor-approved libraries:** scikit-learn, pandas, numpy\n")
            f.write("- **Comprehensive evaluation:** Multiple metrics reported\n")
            f.write("- **Reproducible methodology:** Seeds and parameters documented\n")
            f.write("- **Enhanced monitoring:** Training process fully tracked\n")
            f.write("- **Professional documentation:** Metadata and reports generated\n\n")
            
            f.write("## üéØ Next Steps\n\n")
            f.write("1. **XAI Analysis:** Run explainable AI analysis\n")
            f.write("2. **Hyperparameter Tuning:** Optimize model parameters\n")
            f.write("3. **Ensemble Methods:** Combine model predictions\n")
            f.write("4. **Clinical Validation:** Expert review of predictions\n")
        
        logger.info(f"‚úÖ Enhanced report saved: {report_path}")
    
    def run_complete_training_pipeline(self):
        """Run the complete enhanced training pipeline."""
        logger.info("üöÄ Starting Enhanced Baseline Training Pipeline")
        logger.info("=" * 60)
        
        try:
            # Load data
            X_train, X_val, X_test, y_train, y_val, y_test = self.load_processed_data()
            
            # Train models with monitoring
            models = self.train_and_monitor_models(X_train, X_val, y_train, y_val)
            
            # Evaluate on test set
            test_results = self.evaluate_on_test_set(models, X_test, y_test)
            
            # Save models with metadata
            self.save_models_with_metadata(models, test_results)
            
            # Create monitoring visualizations
            self.create_monitoring_visualizations(models, test_results)
            
            # Generate enhanced report
            self.generate_enhanced_report(models, test_results)
            
            logger.info("=" * 60)
            logger.info("üéâ Enhanced Training Pipeline Completed Successfully!")
            
            # Find best model
            best_model = max(test_results.items(), 
                           key=lambda x: x[1]['metrics']['roc_auc'])
            best_name, best_results = best_model
            
            logger.info(f"üèÜ Best Model: {best_name}")
            logger.info(f"üìä Test AUC: {best_results['metrics']['roc_auc']:.3f}")
            logger.info(f"üíæ Models saved to: {self.models_dir}")
            logger.info(f"üìä Visualizations saved to: {self.figures_dir}")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Enhanced training pipeline failed: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """Main execution function."""
    try:
        trainer = EnhancedBaselineTrainer()
        success = trainer.run_complete_training_pipeline()
        
        if success:
            print("\nüéâ Enhanced baseline training completed successfully!")
            print("üíæ Models saved with comprehensive metadata")
            print("üìä Training monitoring visualizations created")
            print("üìù Enhanced report generated")
            return 0
        else:
            print("\n‚ùå Enhanced baseline training failed. Check logs for details.")
            return 1
            
    except Exception as e:
        logger.error(f"‚ùå Fatal error in main: {str(e)}")
        return 1

if __name__ == "__main__":
    exit(main())
