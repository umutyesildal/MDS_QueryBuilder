#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Academic Compliance Checker for Ãœbung 5 ML Implementation

This script validates all implemented ML tasks against the official requirements
for Sections 6.1, 6.2, and 6.3 to ensure academic compliance.
"""

import sys
import os
import pandas as pd
import numpy as np
from datetime import datetime
import pickle
import json
import logging

# Colors for beautiful output
GREEN = '\033[0;32m'
YELLOW = '\033[1;33m'
BLUE = '\033[0;34m'
CYAN = '\033[0;36m'
RED = '\033[0;31m'
WHITE = '\033[1;37m'
PURPLE = '\033[0;35m'
NC = '\033[0m'

def get_log_path(filename):
    """Get path for log files"""
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    logs_dir = os.path.join(project_root, 'logs')
    os.makedirs(logs_dir, exist_ok=True)
    return os.path.join(logs_dir, filename)

def get_report_path(filename):
    """Get path for report files"""
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    reports_dir = os.path.join(project_root, 'docs', 'reports')
    os.makedirs(reports_dir, exist_ok=True)
    return os.path.join(reports_dir, filename)

def print_header(message):
    print(f"{PURPLE}{'â•”' + 'â•' * 68 + 'â•—'}{NC}")
    print(f"{PURPLE}â•‘{WHITE}{message:^68}{PURPLE}â•‘{NC}")
    print(f"{PURPLE}{'â•š' + 'â•' * 68 + 'â•'}{NC}")

def print_section(message):
    print(f"{CYAN}ğŸ“‹ {message}{NC}")

def print_success(message):
    print(f"{GREEN}âœ… {message}{NC}")

def print_info(message):
    print(f"{YELLOW}ğŸ“Š {message}{NC}")

def print_error(message):
    print(f"{RED}âŒ {message}{NC}")

# Setup project root
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

class UebungTaskChecker:
    """Comprehensive checker for Ãœbung 5 ML implementation tasks"""
    
    def __init__(self):
        self.setup_logging()
        self.results = {
            '6.1': {'tasks': {}, 'score': 0, 'max_score': 0},
            '6.2': {'tasks': {}, 'score': 0, 'max_score': 0},
            '6.3': {'tasks': {}, 'score': 0, 'max_score': 0}
        }
        self.ml_dir = os.path.join(project_root, 'src', 'ml')
        self.data_dir = os.path.join(project_root, 'data')
        self.logs_dir = os.path.join(project_root, 'logs')
        self.reports_dir = os.path.join(project_root, 'docs', 'reports')
        self.viz_dir = os.path.join(project_root, 'docs', 'visualizations')
        
    def setup_logging(self):
        """Setup logging configuration"""
        log_path = get_log_path('ml_task_checker.log')
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_path),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('TaskChecker')
        
    def check_file_exists(self, filepath, description=""):
        """Helper to check if file exists"""
        exists = os.path.exists(filepath)
        if exists:
            self.logger.info(f"âœ… Found: {description} ({filepath})")
        else:
            self.logger.warning(f"âŒ Missing: {description} ({filepath})")
        return exists
        
    def check_task_6_1_data_preparation(self):
        """
        Section 6.1: Vorbereitung der Daten fÃ¼r maschinelles Lernen
        Check data preparation and ML readiness tasks
        """
        self.logger.info("ğŸ” Checking Section 6.1: Data Preparation for ML")
        section = '6.1'
        
        # Task 6.1.1: Define clinically relevant prediction task
        task_name = "Clinical Prediction Task Definition"
        self.logger.info(f"ğŸ“‹ Checking: {task_name}")
        
        # Check if mortality extraction exists and defines prediction task
        mortality_script = os.path.join(self.ml_dir, 'step2_mortality_extraction.py')
        readme_path = os.path.join(self.ml_dir, 'README.md')
        
        task_defined = False
        if self.check_file_exists(mortality_script, "Mortality extraction script"):
            # Read the script to check for prediction task definition
            with open(mortality_script, 'r') as f:
                content = f.read()
                if "48-hour mortality" in content and "prediction" in content:
                    task_defined = True
                    
        if self.check_file_exists(readme_path, "ML README documentation"):
            with open(readme_path, 'r') as f:
                readme_content = f.read()
                if "48-hour ICU mortality prediction" in readme_content:
                    task_defined = True
                    
        self.results[section]['tasks'][task_name] = {
            'completed': task_defined,
            'details': "48-hour ICU mortality prediction task clearly defined" if task_defined else "Prediction task not clearly defined",
            'points': 2 if task_defined else 0,
            'max_points': 2
        }
        self.results[section]['max_score'] += 2
        if task_defined:
            self.results[section]['score'] += 2
            
        # Task 6.1.2: Analyze class distribution
        task_name = "Class Distribution Analysis"
        self.logger.info(f"ğŸ“‹ Checking: {task_name}")
        
        eda_script = os.path.join(self.ml_dir, 'step3_exploratory_analysis.py')
        eda_report = os.path.join(self.reports_dir, 'ml_eda_report.md')
        class_analysis_done = False
        
        if self.check_file_exists(eda_script, "EDA script"):
            with open(eda_script, 'r') as f:
                content = f.read()
                if "class_imbalance_analysis" in content and "imbalance_ratio" in content:
                    class_analysis_done = True
                    
        if self.check_file_exists(eda_report, "EDA report"):
            with open(eda_report, 'r') as f:
                content = f.read()
                if "Class Imbalance" in content and "13.2:1" in content:
                    class_analysis_done = True
                    
        self.results[section]['tasks'][task_name] = {
            'completed': class_analysis_done,
            'details': "Class imbalance (13.2:1) analyzed and documented" if class_analysis_done else "Class distribution analysis missing",
            'points': 2 if class_analysis_done else 0,
            'max_points': 2
        }
        self.results[section]['max_score'] += 2
        if class_analysis_done:
            self.results[section]['score'] += 2
            
        # Task 6.1.3: Temporal data splitting
        task_name = "Temporal Data Splitting"
        self.logger.info(f"ğŸ“‹ Checking: {task_name}")
        
        feature_script = os.path.join(self.ml_dir, 'step4_feature_engineering.py')
        processed_data = os.path.join(self.data_dir, 'processed')
        temporal_split_done = False
        
        if self.check_file_exists(feature_script, "Feature engineering script"):
            with open(feature_script, 'r') as f:
                content = f.read()
                if "temporal_train_test_split" in content and "prediction_timepoint" in content:
                    temporal_split_done = True
                    
        # Check if processed split files exist
        split_files = ['X_train.csv', 'X_val.csv', 'X_test.csv', 'y_train.csv', 'y_val.csv', 'y_test.csv']
        if os.path.exists(processed_data):
            all_splits_exist = all(os.path.exists(os.path.join(processed_data, f)) for f in split_files)
            if all_splits_exist:
                temporal_split_done = True
                
        self.results[section]['tasks'][task_name] = {
            'completed': temporal_split_done,
            'details': "Temporal train/validation/test split implemented" if temporal_split_done else "Temporal splitting not implemented",
            'points': 3 if temporal_split_done else 0,
            'max_points': 3
        }
        self.results[section]['max_score'] += 3
        if temporal_split_done:
            self.results[section]['score'] += 3
            
        # Task 6.1.4: Handle class imbalance strategy
        task_name = "Class Imbalance Handling Strategy"
        self.logger.info(f"ğŸ“‹ Checking: {task_name}")
        
        baseline_script = os.path.join(self.ml_dir, 'step5_baseline_models.py')
        imbalance_handled = False
        
        if self.check_file_exists(baseline_script, "Baseline models script"):
            with open(baseline_script, 'r') as f:
                content = f.read()
                if "SMOTE" in content or "class_weight" in content:
                    imbalance_handled = True
                    
        self.results[section]['tasks'][task_name] = {
            'completed': imbalance_handled,
            'details': "SMOTE or class weighting implemented" if imbalance_handled else "Class imbalance handling missing",
            'points': 2 if imbalance_handled else 0,
            'max_points': 2
        }
        self.results[section]['max_score'] += 2
        if imbalance_handled:
            self.results[section]['score'] += 2
            
    def check_task_6_2_model_implementation(self):
        """
        Section 6.2: Implementierung und Training von ML-Modellen
        Check model implementation and training tasks
        """
        self.logger.info("ğŸ” Checking Section 6.2: Model Implementation & Training")
        section = '6.2'
        
        # Task 6.2.1: Baseline Model Implementation
        task_name = "Baseline Model Implementation"
        self.logger.info(f"ğŸ“‹ Checking: {task_name}")
        
        baseline_script = os.path.join(self.ml_dir, 'step5_baseline_models.py')
        baseline_implemented = False
        baseline_models = []
        
        if self.check_file_exists(baseline_script, "Baseline models script"):
            with open(baseline_script, 'r') as f:
                content = f.read()
                models = ['LogisticRegression', 'RandomForest', 'GradientBoosting', 'SVM']
                found_models = [model for model in models if model in content]
                if len(found_models) >= 2:  # At least 2 baseline models
                    baseline_implemented = True
                    baseline_models = found_models
                    
        self.results[section]['tasks'][task_name] = {
            'completed': baseline_implemented,
            'details': f"Baseline models implemented: {', '.join(baseline_models)}" if baseline_implemented else "Baseline models missing",
            'points': 3 if baseline_implemented else 0,
            'max_points': 3
        }
        self.results[section]['max_score'] += 3
        if baseline_implemented:
            self.results[section]['score'] += 3
            
        # Task 6.2.2: Advanced ML Model Implementation
        task_name = "Advanced ML Model Implementation"
        self.logger.info(f"ğŸ“‹ Checking: {task_name}")
        
        # Check for advanced models (Random Forest, Gradient Boosting are considered advanced)
        advanced_implemented = False
        if baseline_implemented and ('RandomForest' in baseline_models or 'GradientBoosting' in baseline_models):
            advanced_implemented = True
            
        self.results[section]['tasks'][task_name] = {
            'completed': advanced_implemented,
            'details': "Random Forest and Gradient Boosting implemented" if advanced_implemented else "Advanced models missing",
            'points': 3 if advanced_implemented else 0,
            'max_points': 3
        }
        self.results[section]['max_score'] += 3
        if advanced_implemented:
            self.results[section]['score'] += 3
            
        # Task 6.2.3: Model Training with Monitoring
        task_name = "Model Training & Monitoring"
        self.logger.info(f"ğŸ“‹ Checking: {task_name}")
        
        training_implemented = False
        models_saved = False
        
        # Check for training implementation
        if self.check_file_exists(baseline_script, "Training script"):
            with open(baseline_script, 'r') as f:
                content = f.read()
                if "cross_validation" in content or "GridSearchCV" in content:
                    training_implemented = True
                if "pickle.dump" in content or "joblib.dump" in content:
                    models_saved = True
                    
        # Check if models directory exists with saved models
        models_dir = os.path.join(project_root, 'models')
        if os.path.exists(models_dir) and os.listdir(models_dir):
            models_saved = True
            
        training_complete = training_implemented and models_saved
        
        self.results[section]['tasks'][task_name] = {
            'completed': training_complete,
            'details': f"Training: {'âœ…' if training_implemented else 'âŒ'}, Model Saving: {'âœ…' if models_saved else 'âŒ'}",
            'points': 2 if training_complete else 1 if training_implemented else 0,
            'max_points': 2
        }
        self.results[section]['max_score'] += 2
        if training_complete:
            self.results[section]['score'] += 2
        elif training_implemented:
            self.results[section]['score'] += 1
            
        # Task 6.2.4: Model Justification
        task_name = "Model Selection Justification"
        self.logger.info(f"ğŸ“‹ Checking: {task_name}")
        
        justification_found = False
        readme_path = os.path.join(self.ml_dir, 'README.md')
        
        if self.check_file_exists(readme_path, "ML README"):
            with open(readme_path, 'r') as f:
                content = f.read()
                if "Random Forest" in content and "clinical" in content.lower():
                    justification_found = True
                    
        self.results[section]['tasks'][task_name] = {
            'completed': justification_found,
            'details': "Model selection justified in documentation" if justification_found else "Model justification missing",
            'points': 1 if justification_found else 0,
            'max_points': 1
        }
        self.results[section]['max_score'] += 1
        if justification_found:
            self.results[section]['score'] += 1
            
    def check_task_6_3_evaluation_and_xai(self):
        """
        Section 6.3: Modellbewertung und Explainable AI
        Check model evaluation and XAI implementation
        """
        self.logger.info("ğŸ” Checking Section 6.3: Model Evaluation & Explainable AI")
        section = '6.3'
        
        # Task 6.3.1: Rigorous Model Evaluation
        task_name = "Model Performance Evaluation"
        self.logger.info(f"ğŸ“‹ Checking: {task_name}")
        
        enhanced_models_script = os.path.join(self.ml_dir, 'step5_enhanced_models.py')
        baseline_script = os.path.join(self.ml_dir, 'step5_baseline_models.py')
        ml_report = os.path.join(self.reports_dir, 'ml_enhanced_models_report.md')
        
        evaluation_done = False
        metrics_used = []
        
        # Check enhanced models script first
        if self.check_file_exists(enhanced_models_script, "Enhanced models script"):
            with open(enhanced_models_script, 'r') as f:
                content = f.read()
                required_metrics = ['roc_auc_score', 'f1_score', 'precision_score', 'recall_score', 'average_precision_score']
                found_metrics = [metric for metric in required_metrics if metric in content]
                if len(found_metrics) >= 4:  # At least 4 metrics
                    evaluation_done = True
                    metrics_used = found_metrics
        
        # Fallback to baseline script
        elif self.check_file_exists(baseline_script, "Baseline models script"):
            with open(baseline_script, 'r') as f:
                content = f.read()
                required_metrics = ['roc_auc_score', 'f1_score', 'precision_score', 'recall_score']
                found_metrics = [metric for metric in required_metrics if metric in content]
                if len(found_metrics) >= 3:  # At least 3 metrics
                    evaluation_done = True
                    metrics_used = found_metrics
                    
        self.results[section]['tasks'][task_name] = {
            'completed': evaluation_done,
            'details': f"Comprehensive metrics implemented: {', '.join(metrics_used)}" if evaluation_done else "Comprehensive evaluation missing",
            'points': 3 if evaluation_done else 0,
            'max_points': 3
        }
        self.results[section]['max_score'] += 3
        if evaluation_done:
            self.results[section]['score'] += 3
            
        # Task 6.3.2: Explainable AI Implementation
        task_name = "Explainable AI (XAI) Implementation"
        self.logger.info(f"ğŸ“‹ Checking: {task_name}")
        
        # Check for dedicated XAI implementation
        xai_script = os.path.join(self.ml_dir, 'step6_explainable_ai.py')
        xai_report = os.path.join(self.reports_dir, 'ml_xai_report.md')
        xai_viz_dir = os.path.join(self.viz_dir, 'xai')
        
        xai_implemented = False
        xai_methods = []
        xai_score = 0
        
        # Check for dedicated XAI script
        if self.check_file_exists(xai_script, "Dedicated XAI script"):
            with open(xai_script, 'r') as f:
                content = f.read()
                if 'SHAP' in content or 'shap' in content:
                    xai_implemented = True
                    xai_methods.append('SHAP')
                    xai_score += 2
                if 'TreeExplainer' in content and 'LinearExplainer' in content:
                    xai_methods.append('Multiple SHAP Explainers')
                    xai_score += 1
                if 'patient_level' in content.lower() or 'patient' in content.lower():
                    xai_methods.append('Patient-level Explanations')
                    xai_score += 1
        
        # Check for XAI report and visualizations
        if self.check_file_exists(xai_report, "XAI report"):
            xai_score += 1
            
        if os.path.exists(xai_viz_dir) and len(os.listdir(xai_viz_dir)) > 0:
            xai_viz_count = len([f for f in os.listdir(xai_viz_dir) if f.endswith('.png')])
            if xai_viz_count >= 5:
                xai_methods.append(f'{xai_viz_count} XAI visualizations')
                xai_score += 1
                
        # Fallback: check baseline models for basic feature importance
        if not xai_implemented:
            if self.check_file_exists(baseline_script, "Baseline script with feature importance"):
                with open(baseline_script, 'r') as f:
                    content = f.read()
                    if 'feature_importances_' in content:
                        xai_methods.append('Feature Importance')
                        xai_score = 1
                        
        self.results[section]['tasks'][task_name] = {
            'completed': xai_implemented,
            'details': f"XAI methods implemented: {', '.join(xai_methods)}" if xai_methods else "XAI implementation missing",
            'points': min(xai_score, 4),
            'max_points': 4
        }
        self.results[section]['max_score'] += 4
        self.results[section]['score'] += min(xai_score, 4)
            
        # Task 6.3.3: Clinical Patient Examples Analysis
        task_name = "Clinical Patient Examples Analysis"
        self.logger.info(f"ğŸ“‹ Checking: {task_name}")
        
        examples_analyzed = False
        examples_score = 0
        
        # Check for patient-level analysis in XAI
        if os.path.exists(xai_script):
            with open(xai_script, 'r') as f:
                content = f.read()
                if 'patient_level' in content.lower() and 'clinical' in content.lower():
                    examples_analyzed = True
                    examples_score += 2
                if 'true_positive' in content.lower() and 'false_positive' in content.lower():
                    examples_score += 1
        
        # Check for patient visualizations
        if os.path.exists(xai_viz_dir):
            patient_viz = [f for f in os.listdir(xai_viz_dir) if 'patient' in f.lower() or 'explanation' in f.lower()]
            if len(patient_viz) >= 3:
                examples_analyzed = True
                examples_score = max(examples_score, 2)
                
        # Fallback: check general visualizations
        viz_files = []
        if os.path.exists(self.viz_dir):
            for root, dirs, files in os.walk(self.viz_dir):
                viz_files.extend([os.path.join(root, f) for f in files if f.endswith('.png')])
                
        if len(viz_files) > 10:  # Many visualizations suggest comprehensive analysis
            examples_score = max(examples_score, 1)
            
        self.results[section]['tasks'][task_name] = {
            'completed': examples_analyzed,
            'details': f"Patient analysis implemented with {len(viz_files)} total visualizations" if examples_score > 0 else "Patient examples analysis missing",
            'points': examples_score,
            'max_points': 3
        }
        self.results[section]['max_score'] += 3
        self.results[section]['score'] += examples_score
            
        # Task 6.3.4: Clinical Usefulness Assessment
        task_name = "Clinical Usefulness Assessment"
        self.logger.info(f"ğŸ“‹ Checking: {task_name}")
        
        clinical_assessment = False
        assessment_score = 0
        
        # Check XAI report for clinical discussion
        if self.check_file_exists(xai_report, "XAI report"):
            with open(xai_report, 'r') as f:
                content = f.read()
                if "clinical" in content.lower() and ("decision" in content.lower() or "usefulness" in content.lower()):
                    clinical_assessment = True
                    assessment_score += 1
                if "recommendations" in content.lower():
                    assessment_score += 1
                    
        # Check main README for clinical context
        readme_path = os.path.join(self.ml_dir, 'README.md')
        if self.check_file_exists(readme_path, "ML README"):
            with open(readme_path, 'r') as f:
                content = f.read()
                if "clinical" in content.lower() and ("decision" in content.lower() or "usefulness" in content.lower()):
                    clinical_assessment = True
                    assessment_score = max(assessment_score, 1)
                    
        self.results[section]['tasks'][task_name] = {
            'completed': clinical_assessment,
            'details': "Clinical usefulness discussed in documentation" if clinical_assessment else "Clinical assessment missing",
            'points': assessment_score,
            'max_points': 2
        }
        self.results[section]['max_score'] += 2
        self.results[section]['score'] += assessment_score
            
    def check_implementation_quality(self):
        """Check overall implementation quality and academic standards"""
        self.logger.info("ğŸ” Checking Implementation Quality & Academic Standards")
        
        quality_checks = {
            'Code Organization': self.check_code_organization(),
            'Documentation Quality': self.check_documentation_quality(),
            'Logging & Reporting': self.check_logging_reporting(),
            'Reproducibility': self.check_reproducibility(),
            'Academic Standards': self.check_academic_standards()
        }
        
        return quality_checks
        
    def check_code_organization(self):
        """Check code organization and structure"""
        expected_files = [
            'step1_environment_setup.py',
            'step2_mortality_extraction.py',
            'step3_exploratory_analysis.py',
            'step4_feature_engineering.py',
            'step5_baseline_models.py',
            'README.md',
            'requirements_ml.txt'
        ]
        
        existing_files = [f for f in expected_files if os.path.exists(os.path.join(self.ml_dir, f))]
        organization_score = len(existing_files) / len(expected_files)
        
        return {
            'score': organization_score,
            'details': f"{len(existing_files)}/{len(expected_files)} expected files present",
            'files': existing_files
        }
        
    def check_documentation_quality(self):
        """Check documentation completeness"""
        readme_path = os.path.join(self.ml_dir, 'README.md')
        
        if not os.path.exists(readme_path):
            return {'score': 0, 'details': "README.md missing"}
            
        with open(readme_path, 'r') as f:
            content = f.read()
            
        quality_indicators = [
            'Objective' in content,
            'Requirements' in content,
            'Pipeline Architecture' in content,
            'Quick Start' in content,
            'Results' in content,
            'Next Steps' in content
        ]
        
        score = sum(quality_indicators) / len(quality_indicators)
        
        return {
            'score': score,
            'details': f"{sum(quality_indicators)}/{len(quality_indicators)} documentation sections present"
        }
        
    def check_logging_reporting(self):
        """Check logging and reporting implementation"""
        log_files = []
        report_files = []
        
        if os.path.exists(self.logs_dir):
            log_files = [f for f in os.listdir(self.logs_dir) if f.startswith('ml_') and f.endswith('.log')]
            
        if os.path.exists(self.reports_dir):
            report_files = [f for f in os.listdir(self.reports_dir) if f.startswith('ml_') and f.endswith('.md')]
            
        total_expected = 5  # 5 ML steps should generate logs/reports
        actual_logs = len(log_files)
        actual_reports = len(report_files)
        
        score = (actual_logs + actual_reports) / (2 * total_expected)
        
        return {
            'score': min(score, 1.0),
            'details': f"Logs: {actual_logs}, Reports: {actual_reports}",
            'log_files': log_files,
            'report_files': report_files
        }
        
    def check_reproducibility(self):
        """Check reproducibility features"""
        reproducibility_features = []
        
        # Check for requirements file
        if os.path.exists(os.path.join(self.ml_dir, 'requirements_ml.txt')):
            reproducibility_features.append('Requirements file')
            
        # Check for saved processed data
        if os.path.exists(os.path.join(self.data_dir, 'processed')):
            reproducibility_features.append('Processed data saved')
            
        # Check for saved models
        if os.path.exists(os.path.join(project_root, 'models')):
            reproducibility_features.append('Models saved')
            
        # Check for metadata/scalers
        processed_dir = os.path.join(self.data_dir, 'processed')
        if os.path.exists(os.path.join(processed_dir, 'metadata.pickle')):
            reproducibility_features.append('Metadata preserved')
            
        if os.path.exists(os.path.join(processed_dir, 'scalers.pickle')):
            reproducibility_features.append('Scalers saved')
            
        score = len(reproducibility_features) / 5
        
        return {
            'score': score,
            'details': f"{len(reproducibility_features)}/5 reproducibility features",
            'features': reproducibility_features
        }
        
    def check_academic_standards(self):
        """Check adherence to academic standards"""
        standards_met = []
        
        # Check for proper citations/references in README
        readme_path = os.path.join(self.ml_dir, 'README.md')
        if os.path.exists(readme_path):
            with open(readme_path, 'r') as f:
                content = f.read()
                if 'References' in content:
                    standards_met.append('References provided')
                if 'limitations' in content.lower() or 'known issues' in content.lower():
                    standards_met.append('Limitations discussed')
                if 'professor approved' in content.lower():
                    standards_met.append('Library restrictions followed')
                    
        # Check for step-by-step implementation
        ml_files = [f for f in os.listdir(self.ml_dir) if f.startswith('step') and f.endswith('.py')]
        if len(ml_files) >= 5:
            standards_met.append('Step-by-step implementation')
            
        score = len(standards_met) / 4
        
        return {
            'score': score,
            'details': f"{len(standards_met)}/4 academic standards met",
            'standards': standards_met
        }
        
    def generate_comprehensive_report(self):
        """Generate comprehensive task completion report"""
        total_score = sum(section['score'] for section in self.results.values())
        total_max_score = sum(section['max_score'] for section in self.results.values())
        overall_percentage = (total_score / total_max_score * 100) if total_max_score > 0 else 0
        
        # Check implementation quality
        quality_checks = self.check_implementation_quality()
        
        report_path = get_report_path('ml_task_checker_report.md')
        
        with open(report_path, 'w') as f:
            f.write("# ğŸ“‹ Ãœbung 5 - ML Task Completion Report\n\n")
            f.write(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## ğŸ¯ Overall Completion Status\n\n")
            f.write(f"**Total Score:** {total_score}/{total_max_score} ({overall_percentage:.1f}%)\n\n")
            
            # Determine overall status
            if overall_percentage >= 90:
                status = "ğŸŸ¢ EXCELLENT"
            elif overall_percentage >= 80:
                status = "ğŸŸ¡ GOOD"  
            elif overall_percentage >= 70:
                status = "ğŸŸ  SATISFACTORY"
            elif overall_percentage >= 60:
                status = "ğŸ”´ NEEDS IMPROVEMENT"
            else:
                status = "âŒ INCOMPLETE"
                
            f.write(f"**Status:** {status}\n\n")
            
            # Section-by-section breakdown
            for section_num, section_data in self.results.items():
                section_percentage = (section_data['score'] / section_data['max_score'] * 100) if section_data['max_score'] > 0 else 0
                f.write(f"## ğŸ“Š Section {section_num} Results ({section_percentage:.1f}%)\n\n")
                
                if section_num == '6.1':
                    f.write("**Focus:** Data Preparation for ML\n\n")
                elif section_num == '6.2':
                    f.write("**Focus:** Model Implementation & Training\n\n")
                elif section_num == '6.3':
                    f.write("**Focus:** Model Evaluation & Explainable AI\n\n")
                    
                f.write(f"**Score:** {section_data['score']}/{section_data['max_score']}\n\n")
                
                for task_name, task_data in section_data['tasks'].items():
                    status_icon = "âœ…" if task_data['completed'] else "âŒ"
                    f.write(f"### {status_icon} {task_name}\n")
                    f.write(f"**Points:** {task_data['points']}/{task_data['max_points']}\n")
                    f.write(f"**Details:** {task_data['details']}\n\n")
                    
            # Implementation Quality Assessment
            f.write("## ğŸ”§ Implementation Quality Assessment\n\n")
            
            for check_name, check_data in quality_checks.items():
                score_percentage = check_data['score'] * 100
                status_icon = "âœ…" if score_percentage >= 80 else "ğŸŸ¡" if score_percentage >= 60 else "âŒ"
                f.write(f"### {status_icon} {check_name} ({score_percentage:.1f}%)\n")
                f.write(f"**Details:** {check_data['details']}\n\n")
                
            # Recommendations
            f.write("## ğŸ’¡ Recommendations\n\n")
            
            if overall_percentage < 100:
                f.write("### Next Steps to Complete Implementation:\n")
                
                # Check what's missing
                missing_tasks = []
                for section_num, section_data in self.results.items():
                    for task_name, task_data in section_data['tasks'].items():
                        if not task_data['completed']:
                            missing_tasks.append(f"**{section_num}:** {task_name}")
                            
                if missing_tasks:
                    for task in missing_tasks:
                        f.write(f"- {task}\n")
                        
                f.write("\n### Priority Actions:\n")
                if overall_percentage < 70:
                    f.write("1. ğŸ”¥ **HIGH PRIORITY:** Complete Section 6.1 data preparation tasks\n")
                    f.write("2. ğŸ”¥ **HIGH PRIORITY:** Implement baseline models in Section 6.2\n")
                    f.write("3. ğŸ“ˆ **MEDIUM:** Add comprehensive evaluation metrics\n")
                    f.write("4. ğŸ¯ **LOW:** Implement advanced XAI features\n")
                else:
                    f.write("1. ğŸ“ˆ **MEDIUM:** Complete remaining evaluation metrics\n")
                    f.write("2. ğŸ¯ **LOW:** Implement SHAP/LIME for explainable AI\n")
                    f.write("3. ğŸ“ **LOW:** Add clinical patient examples analysis\n")
                    
            else:
                f.write("ğŸ‰ **All core tasks completed!** Consider implementing advanced features:\n")
                f.write("- Advanced time-series models with Darts\n")
                f.write("- Comprehensive SHAP analysis\n")
                f.write("- Clinical decision support interface\n")
                
            f.write("\n## ğŸ“š Academic Compliance\n\n")
            f.write("âœ… Professor-approved libraries only\n")
            f.write("âœ… Step-by-step implementation approach\n")
            f.write("âœ… Comprehensive documentation\n")
            f.write("âœ… Reproducible methodology\n")
            
        self.logger.info(f"ğŸ“„ Comprehensive report saved: {report_path}")
        return report_path, overall_percentage
        
    def print_summary(self):
        """Print a concise, human-readable summary to console"""
        total_score = sum(section['score'] for section in self.results.values())
        total_max_score = sum(section['max_score'] for section in self.results.values())
        overall_percentage = (total_score / total_max_score * 100) if total_max_score > 0 else 0
        
        print("\n" + "="*80)
        print("ğŸ“ ÃœBUNG 5 - ML IMPLEMENTATION COMPLETION STATUS")
        print("="*80)
        print(f"ğŸ¯ Overall Progress: {total_score}/{total_max_score} points ({overall_percentage:.1f}%)")
        print()
        
        # Progress bar
        bar_length = 50
        filled_length = int(bar_length * overall_percentage / 100)
        bar = "â–ˆ" * filled_length + "â–‘" * (bar_length - filled_length)
        print(f"Progress: [{bar}] {overall_percentage:.1f}%")
        print()
        
        # Section breakdown with detailed status
        section_names = {
            '6.1': 'Data Preparation for ML',
            '6.2': 'Model Implementation & Training', 
            '6.3': 'Model Evaluation & Explainable AI'
        }
        
        for section_num, section_data in self.results.items():
            section_percentage = (section_data['score'] / section_data['max_score'] * 100) if section_data['max_score'] > 0 else 0
            
            if section_percentage >= 90:
                status_emoji = "ğŸŸ¢"
                status_text = "EXCELLENT"
            elif section_percentage >= 80:
                status_emoji = "ğŸŸ¡"
                status_text = "GOOD"
            elif section_percentage >= 70:
                status_emoji = "ï¿½"
                status_text = "SATISFACTORY"
            elif section_percentage >= 50:
                status_emoji = "ğŸ”´"
                status_text = "NEEDS WORK"
            else:
                status_emoji = "âŒ"
                status_text = "INCOMPLETE"
            
            print(f"{status_emoji} Section {section_num}: {section_names[section_num]}")
            print(f"   Score: {section_data['score']}/{section_data['max_score']} ({section_percentage:.1f}%) - {status_text}")
            
            # Show completed tasks
            completed_tasks = [task for task, data in section_data['tasks'].items() if data['completed']]
            incomplete_tasks = [task for task, data in section_data['tasks'].items() if not data['completed']]
            
            if completed_tasks:
                print(f"   âœ… Completed: {len(completed_tasks)} tasks")
                for task in completed_tasks[:2]:  # Show first 2
                    print(f"      â€¢ {task}")
                if len(completed_tasks) > 2:
                    print(f"      â€¢ ... and {len(completed_tasks) - 2} more")
            
            if incomplete_tasks:
                print(f"   âŒ Remaining: {len(incomplete_tasks)} tasks")
                for task in incomplete_tasks[:2]:  # Show first 2
                    print(f"      â€¢ {task}")
                if len(incomplete_tasks) > 2:
                    print(f"      â€¢ ... and {len(incomplete_tasks) - 2} more")
            print()
        
        print("="*80)
        
        # Overall status with actionable advice
        if overall_percentage >= 95:
            print("ğŸ† STATUS: OUTSTANDING - Ready for presentation!")
            print("ï¿½ Suggestion: Consider adding advanced features for excellence")
        elif overall_percentage >= 85:
            print("ğŸ‰ STATUS: EXCELLENT - Almost perfect implementation!")
            print("ğŸ’¡ Suggestion: Polish remaining details for perfection")
        elif overall_percentage >= 75:
            print("ï¿½ STATUS: GOOD - Solid implementation with minor gaps")
            print("ğŸ’¡ Suggestion: Focus on completing XAI and patient analysis")
        elif overall_percentage >= 65:
            print("ï¿½ STATUS: SATISFACTORY - Core requirements mostly met")
            print("ğŸ’¡ Suggestion: Enhance evaluation metrics and add XAI features")
        elif overall_percentage >= 50:
            print("âš ï¸  STATUS: NEEDS IMPROVEMENT - Missing key components")
            print("ğŸ’¡ Suggestion: Focus on model training and basic evaluation first")
        else:
            print("ğŸš¨ STATUS: INCOMPLETE - Significant work needed")
            print("ğŸ’¡ Suggestion: Start with data preparation and basic models")
        
        print()
        print("ğŸ“Š Implementation Highlights:")
        
        # Count various achievements
        total_tasks = sum(len(section['tasks']) for section in self.results.values())
        completed_tasks = sum(len([t for t in section['tasks'].values() if t['completed']]) 
                             for section in self.results.values())
        
        print(f"   âœ… Tasks Completed: {completed_tasks}/{total_tasks}")
        
        # Check for specific implementations
        xai_dir = os.path.join(self.viz_dir, 'xai')
        if os.path.exists(xai_dir) and len(os.listdir(xai_dir)) > 0:
            xai_viz_count = len([f for f in os.listdir(xai_dir) if f.endswith('.png')])
            print(f"   ğŸ” XAI Visualizations: {xai_viz_count} files generated")
        
        if os.path.exists(os.path.join(project_root, 'models')) and len(os.listdir(os.path.join(project_root, 'models'))) > 0:
            model_count = len([f for f in os.listdir(os.path.join(project_root, 'models')) if f.endswith('.pkl')])
            print(f"   ğŸ¤– Models Trained: {model_count} models saved")
        
        total_viz = 0
        if os.path.exists(self.viz_dir):
            for root, dirs, files in os.walk(self.viz_dir):
                total_viz += len([f for f in files if f.endswith('.png')])
        print(f"   ğŸ“Š Total Visualizations: {total_viz} files")
        
        total_reports = 0
        if os.path.exists(self.reports_dir):
            total_reports = len([f for f in os.listdir(self.reports_dir) 
                               if f.startswith('ml_') and f.endswith('.md')])
        print(f"   ğŸ“„ Reports Generated: {total_reports} reports")
        
        print("="*80)
        print("ğŸ“‹ Next Steps:")
        
        if overall_percentage < 100:
            if overall_percentage < 70:
                print("ğŸ”¥ HIGH PRIORITY:")
                print("   1. Complete basic model training and evaluation")
                print("   2. Implement comprehensive metrics (AUC, F1, Precision, Recall)")
                print("   3. Add class imbalance handling (SMOTE)")
            elif overall_percentage < 85:
                print("ğŸ“ˆ MEDIUM PRIORITY:")
                print("   1. Implement comprehensive XAI with SHAP")
                print("   2. Add patient-level explanations")
                print("   3. Create clinical usefulness assessment")
            else:
                print("ğŸ¯ LOW PRIORITY (Polish for excellence):")
                print("   1. Add advanced time-series models")
                print("   2. Implement clinical decision support interface")
                print("   3. Add cross-validation and robustness testing")
        else:
            print("ğŸ‰ ALL REQUIREMENTS COMPLETED!")
            print("   Consider these advanced features:")
            print("   â€¢ Advanced ensemble methods")
            print("   â€¢ Real-time prediction interface")
            print("   â€¢ External validation studies")
        
        print("\nğŸ“š Resources:")
        print("   â€¢ ğŸ“– Detailed report: docs/reports/ml_task_checker_report.md")
        print("   â€¢ ğŸ” XAI visualizations: docs/visualizations/xai/")
        print("   â€¢ ğŸ¤– Trained models: models/")
        print("   â€¢ ğŸ“Š All visualizations: docs/visualizations/")
        print("="*80)

def main():
    """Main execution function"""
    print("ğŸš€ Starting Ãœbung 5 ML Task Completion Check")
    print("=" * 60)
    
    try:
        checker = UebungTaskChecker()
        
        # Check all sections
        checker.check_task_6_1_data_preparation()
        checker.check_task_6_2_model_implementation()
        checker.check_task_6_3_evaluation_and_xai()
        
        # Generate comprehensive report
        report_path, overall_percentage = checker.generate_comprehensive_report()
        
        # Print summary
        checker.print_summary()
        
        print(f"\nğŸ“„ Detailed report saved: {report_path}")
        print(f"ğŸ“Š Overall completion: {overall_percentage:.1f}%")
        
        return 0 if overall_percentage >= 70 else 1
        
    except Exception as e:
        print(f"âŒ Task checking failed: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())
