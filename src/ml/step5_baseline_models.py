#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Uebung 5: ML Implementation - Step 5: Baseline Model Implementation
Medical Data Science - ICU Mortality Prediction using SOFA Scores

This script implements baseline models with class imbalance handling for 
48-hour ICU mortality prediction using only professor-recommended libraries.
"""

import sys
import os
import pandas as pd
import numpy as np
import pickle
import logging
from datetime import datetime

# Scikit-learn imports (professor recommended)
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (classification_report, confusion_matrix, 
                           roc_auc_score, average_precision_score, roc_curve, 
                           precision_recall_curve, f1_score)
from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import SMOTE
from imblearn.combine import SMOTETomek
import matplotlib.pyplot as plt
import seaborn as sns

# SHAP for explainability (professor recommended)
import shap

# Project imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.file_paths import get_log_path, get_report_path

# Setup project root
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

class BaselineModelTrainer:
    """Baseline model training with class imbalance handling"""
    
    def __init__(self):
        self.setup_logging()
        self.models = {}
        self.results = {}
        self.processed_dir = os.path.join(project_root, 'data', 'processed')
        self.models_dir = os.path.join(project_root, 'models')
        self.figures_dir = os.path.join(project_root, 'docs', 'visualizations', 'models')
        
        # Create directories
        os.makedirs(self.models_dir, exist_ok=True)
        os.makedirs(self.figures_dir, exist_ok=True)
        
        # Set plotting style
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        
    def setup_logging(self):
        """Setup logging configuration"""
        log_path = get_log_path('ml_baseline_models.log')
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_path),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('BaselineTrainer')
        
    def load_processed_data(self):
        """Load processed train/validation/test data"""
        self.logger.info("üìÇ Loading processed data...")
        
        # Load datasets
        X_train = pd.read_csv(os.path.join(self.processed_dir, 'X_train.csv'))
        X_val = pd.read_csv(os.path.join(self.processed_dir, 'X_val.csv'))
        X_test = pd.read_csv(os.path.join(self.processed_dir, 'X_test.csv'))
        
        y_train = pd.read_csv(os.path.join(self.processed_dir, 'y_train.csv')).iloc[:, 0]
        y_val = pd.read_csv(os.path.join(self.processed_dir, 'y_val.csv')).iloc[:, 0]
        y_test = pd.read_csv(os.path.join(self.processed_dir, 'y_test.csv')).iloc[:, 0]
        
        # Load metadata
        with open(os.path.join(self.processed_dir, 'metadata.pickle'), 'rb') as f:
            self.metadata = pickle.load(f)
            
        self.logger.info(f"‚úÖ Data loaded - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}")\n        \n        return X_train, X_val, X_test, y_train, y_val, y_test\n        \n    def handle_class_imbalance(self, X_train, y_train):\n        \"\"\"Handle class imbalance using multiple strategies\"\"\"\n        self.logger.info(\"‚öñÔ∏è Handling class imbalance...\")\n        \n        strategies = {}\n        \n        # Strategy 1: Class weights\n        class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n        class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n        strategies['class_weights'] = class_weight_dict\n        \n        # Strategy 2: SMOTE oversampling\n        smote = SMOTE(random_state=42, k_neighbors=3)  # Use fewer neighbors due to small dataset\n        X_smote, y_smote = smote.fit_resample(X_train, y_train)\n        strategies['smote'] = (X_smote, y_smote)\n        \n        # Strategy 3: SMOTE + Tomek Links (hybrid)\n        smote_tomek = SMOTETomek(random_state=42, smote=SMOTE(k_neighbors=3))\n        X_smote_tomek, y_smote_tomek = smote_tomek.fit_resample(X_train, y_train)\n        strategies['smote_tomek'] = (X_smote_tomek, y_smote_tomek)\n        \n        # Log results\n        self.logger.info(f\"üìä Original distribution: {np.bincount(y_train)}\")\n        self.logger.info(f\"üìä SMOTE distribution: {np.bincount(y_smote)}\")\n        self.logger.info(f\"üìä SMOTE+Tomek distribution: {np.bincount(y_smote_tomek)}\")\n        self.logger.info(f\"üìä Class weights: {class_weight_dict}\")\n        \n        return strategies\n        \n    def train_baseline_models(self, strategies, X_train, X_val, y_train, y_val):\n        \"\"\"Train baseline models with different imbalance handling strategies\"\"\"\n        self.logger.info(\"ü§ñ Training baseline models...\")\n        \n        # Model configurations\n        model_configs = {\n            'logistic_regression': LogisticRegression(random_state=42, max_iter=1000),\n            'random_forest': RandomForestClassifier(random_state=42, n_estimators=100)\n        }\n        \n        results = {}\n        \n        for model_name, base_model in model_configs.items():\n            self.logger.info(f\"üîß Training {model_name}...\")\n            \n            model_results = {}\n            \n            # 1. No imbalance handling (baseline)\n            model_no_balance = type(base_model)(**base_model.get_params())\n            model_no_balance.fit(X_train, y_train)\n            val_pred_no_balance = model_no_balance.predict_proba(X_val)[:, 1]\n            model_results['no_balance'] = {\n                'model': model_no_balance,\n                'val_predictions': val_pred_no_balance,\n                'strategy': 'none'\n            }\n            \n            # 2. Class weights\n            if hasattr(base_model, 'class_weight'):\n                model_weights = type(base_model)(**{**base_model.get_params(), 'class_weight': strategies['class_weights']})\n                model_weights.fit(X_train, y_train)\n                val_pred_weights = model_weights.predict_proba(X_val)[:, 1]\n                model_results['class_weights'] = {\n                    'model': model_weights,\n                    'val_predictions': val_pred_weights,\n                    'strategy': 'class_weights'\n                }\n            \n            # 3. SMOTE\n            X_smote, y_smote = strategies['smote']\n            model_smote = type(base_model)(**base_model.get_params())\n            model_smote.fit(X_smote, y_smote)\n            val_pred_smote = model_smote.predict_proba(X_val)[:, 1]\n            model_results['smote'] = {\n                'model': model_smote,\n                'val_predictions': val_pred_smote,\n                'strategy': 'smote'\n            }\n            \n            # 4. SMOTE + Tomek\n            X_smote_tomek, y_smote_tomek = strategies['smote_tomek']\n            model_smote_tomek = type(base_model)(**base_model.get_params())\n            model_smote_tomek.fit(X_smote_tomek, y_smote_tomek)\n            val_pred_smote_tomek = model_smote_tomek.predict_proba(X_val)[:, 1]\n            model_results['smote_tomek'] = {\n                'model': model_smote_tomek,\n                'val_predictions': val_pred_smote_tomek,\n                'strategy': 'smote_tomek'\n            }\n            \n            results[model_name] = model_results\n            \n        self.models = results\n        return results\n        \n    def evaluate_models(self, models, X_val, y_val, X_test, y_test):\n        \"\"\"Comprehensive model evaluation\"\"\"\n        self.logger.info(\"üìä Evaluating models...\")\n        \n        evaluation_results = {}\n        \n        for model_name, model_variants in models.items():\n            self.logger.info(f\"üìà Evaluating {model_name}...\")\n            \n            variant_results = {}\n            \n            for variant_name, variant_info in model_variants.items():\n                model = variant_info['model']\n                strategy = variant_info['strategy']\n                \n                # Validation predictions\n                val_pred_proba = model.predict_proba(X_val)[:, 1]\n                val_pred_binary = (val_pred_proba >= 0.5).astype(int)\n                \n                # Test predictions\n                test_pred_proba = model.predict_proba(X_test)[:, 1]\n                test_pred_binary = (test_pred_proba >= 0.5).astype(int)\n                \n                # Calculate metrics\n                val_metrics = self.calculate_metrics(y_val, val_pred_binary, val_pred_proba)\n                test_metrics = self.calculate_metrics(y_test, test_pred_binary, test_pred_proba)\n                \n                variant_results[variant_name] = {\n                    'model': model,\n                    'strategy': strategy,\n                    'val_metrics': val_metrics,\n                    'test_metrics': test_metrics,\n                    'val_pred_proba': val_pred_proba,\n                    'test_pred_proba': test_pred_proba\n                }\n                \n                # Log key metrics\n                self.logger.info(f\"  {variant_name} - Val AUC: {val_metrics['roc_auc']:.3f}, Test AUC: {test_metrics['roc_auc']:.3f}\")\n                \n            evaluation_results[model_name] = variant_results\n            \n        self.results = evaluation_results\n        return evaluation_results\n        \n    def calculate_metrics(self, y_true, y_pred_binary, y_pred_proba):\n        \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n        metrics = {\n            'accuracy': (y_pred_binary == y_true).mean(),\n            'precision': np.sum((y_pred_binary == 1) & (y_true == 1)) / max(np.sum(y_pred_binary == 1), 1),\n            'recall': np.sum((y_pred_binary == 1) & (y_true == 1)) / max(np.sum(y_true == 1), 1),\n            'f1': f1_score(y_true, y_pred_binary),\n            'roc_auc': roc_auc_score(y_true, y_pred_proba),\n            'pr_auc': average_precision_score(y_true, y_pred_proba)\n        }\n        \n        # Handle edge cases\n        if np.sum(y_true == 1) == 0:  # No positive cases\n            metrics['recall'] = 0\n            metrics['pr_auc'] = 0\n            \n        return metrics\n        \n    def select_best_models(self):\n        \"\"\"Select best performing models based on validation metrics\"\"\"\n        self.logger.info(\"üèÜ Selecting best models...\")\n        \n        best_models = {}\n        \n        for model_name, variants in self.results.items():\n            # Select best variant based on validation ROC-AUC\n            best_variant = max(variants.items(), \n                             key=lambda x: x[1]['val_metrics']['roc_auc'])\n            \n            best_models[model_name] = {\n                'variant_name': best_variant[0],\n                'model': best_variant[1]['model'],\n                'strategy': best_variant[1]['strategy'],\n                'val_auc': best_variant[1]['val_metrics']['roc_auc'],\n                'test_auc': best_variant[1]['test_metrics']['roc_auc']\n            }\n            \n            self.logger.info(f\"üèÜ Best {model_name}: {best_variant[0]} (Val AUC: {best_variant[1]['val_metrics']['roc_auc']:.3f})\")\n            \n        # Overall best model\n        overall_best = max(best_models.items(), \n                          key=lambda x: x[1]['val_auc'])\n        \n        self.logger.info(f\"ü•á Overall best model: {overall_best[0]} - {overall_best[1]['variant_name']}\")\n        \n        return best_models, overall_best\n        \n    def create_evaluation_visualizations(self, best_models):\n        \"\"\"Create comprehensive evaluation visualizations\"\"\"\n        self.logger.info(\"üìä Creating evaluation visualizations...\")\n        \n        # 1. ROC Curves Comparison\n        plt.figure(figsize=(12, 8))\n        \n        colors = ['blue', 'red', 'green', 'orange']\n        \n        for i, (model_name, model_info) in enumerate(best_models.items()):\n            # Get test predictions from results\n            variant_name = model_info['variant_name']\n            test_pred_proba = self.results[model_name][variant_name]['test_pred_proba']\n            \n            # Load test data for true labels\n            y_test = pd.read_csv(os.path.join(self.processed_dir, 'y_test.csv')).iloc[:, 0]\n            \n            fpr, tpr, _ = roc_curve(y_test, test_pred_proba)\n            auc_score = model_info['test_auc']\n            \n            plt.plot(fpr, tpr, color=colors[i % len(colors)], \n                    label=f'{model_name} ({model_info[\"strategy\"]}) - AUC: {auc_score:.3f}')\n                    \n        plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('ROC Curves - Best Models Comparison')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.savefig(os.path.join(self.figures_dir, 'roc_curves_comparison.png'), dpi=300, bbox_inches='tight')\n        plt.show()\n        \n        # 2. Precision-Recall Curves\n        plt.figure(figsize=(12, 8))\n        \n        for i, (model_name, model_info) in enumerate(best_models.items()):\n            variant_name = model_info['variant_name']\n            test_pred_proba = self.results[model_name][variant_name]['test_pred_proba']\n            y_test = pd.read_csv(os.path.join(self.processed_dir, 'y_test.csv')).iloc[:, 0]\n            \n            precision, recall, _ = precision_recall_curve(y_test, test_pred_proba)\n            pr_auc = self.results[model_name][variant_name]['test_metrics']['pr_auc']\n            \n            plt.plot(recall, precision, color=colors[i % len(colors)], \n                    label=f'{model_name} ({model_info[\"strategy\"]}) - PR-AUC: {pr_auc:.3f}')\n                    \n        plt.xlabel('Recall')\n        plt.ylabel('Precision')\n        plt.title('Precision-Recall Curves - Best Models Comparison')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.savefig(os.path.join(self.figures_dir, 'pr_curves_comparison.png'), dpi=300, bbox_inches='tight')\n        plt.show()\n        \n        # 3. Metrics Comparison Heatmap\n        metrics_data = []\n        model_labels = []\n        \n        for model_name, model_info in best_models.items():\n            variant_name = model_info['variant_name']\n            test_metrics = self.results[model_name][variant_name]['test_metrics']\n            \n            metrics_data.append([\n                test_metrics['accuracy'],\n                test_metrics['precision'],\n                test_metrics['recall'],\n                test_metrics['f1'],\n                test_metrics['roc_auc'],\n                test_metrics['pr_auc']\n            ])\n            model_labels.append(f\"{model_name}\\n({model_info['strategy']})\")\n            \n        metrics_df = pd.DataFrame(metrics_data, \n                                index=model_labels,\n                                columns=['Accuracy', 'Precision', 'Recall', 'F1', 'ROC-AUC', 'PR-AUC'])\n        \n        plt.figure(figsize=(10, 6))\n        sns.heatmap(metrics_df, annot=True, cmap='YlOrRd', fmt='.3f', cbar_kws={'label': 'Score'})\n        plt.title('Model Performance Metrics Comparison')\n        plt.tight_layout()\n        plt.savefig(os.path.join(self.figures_dir, 'metrics_heatmap.png'), dpi=300, bbox_inches='tight')\n        plt.show()\n        \n    def explain_best_model(self, best_models, overall_best):\n        \"\"\"Use SHAP to explain the best model\"\"\"\n        self.logger.info(\"üîç Generating model explanations with SHAP...\")\n        \n        # Get the overall best model\n        best_model_name = overall_best[0]\n        best_variant_name = overall_best[1]['variant_name']\n        best_model = self.results[best_model_name][best_variant_name]['model']\n        \n        # Load test data\n        X_test = pd.read_csv(os.path.join(self.processed_dir, 'X_test.csv'))\n        \n        # Create SHAP explainer\n        if 'random_forest' in best_model_name:\n            explainer = shap.TreeExplainer(best_model)\n            shap_values = explainer.shap_values(X_test)\n            shap_values = shap_values[1] if isinstance(shap_values, list) else shap_values\n        else:\n            explainer = shap.LinearExplainer(best_model, X_test)\n            shap_values = explainer.shap_values(X_test)\n            \n        # Summary plot\n        plt.figure(figsize=(12, 8))\n        shap.summary_plot(shap_values, X_test, plot_type=\"bar\", show=False)\n        plt.title(f'Feature Importance - {best_model_name} ({best_variant_name})')\n        plt.tight_layout()\n        plt.savefig(os.path.join(self.figures_dir, 'shap_feature_importance.png'), dpi=300, bbox_inches='tight')\n        plt.show()\n        \n        # Detailed summary plot\n        plt.figure(figsize=(12, 10))\n        shap.summary_plot(shap_values, X_test, show=False)\n        plt.title(f'SHAP Summary Plot - {best_model_name} ({best_variant_name})')\n        plt.tight_layout()\n        plt.savefig(os.path.join(self.figures_dir, 'shap_summary_plot.png'), dpi=300, bbox_inches='tight')\n        plt.show()\n        \n        self.logger.info(\"‚úÖ SHAP explanations generated\")\n        \n    def save_models(self, best_models):\n        \"\"\"Save the best models\"\"\"\n        self.logger.info(\"üíæ Saving best models...\")\n        \n        for model_name, model_info in best_models.items():\n            model_filename = f\"{model_name}_{model_info['variant_name']}_best.pickle\"\n            model_path = os.path.join(self.models_dir, model_filename)\n            \n            with open(model_path, 'wb') as f:\n                pickle.dump(model_info['model'], f)\n                \n            self.logger.info(f\"‚úÖ Saved {model_name} to {model_path}\")\n            \n        # Save results\n        results_path = os.path.join(self.models_dir, 'model_evaluation_results.pickle')\n        with open(results_path, 'wb') as f:\n            pickle.dump(self.results, f)\n            \n        self.logger.info(f\"‚úÖ Saved evaluation results to {results_path}\")\n        \n    def generate_final_report(self, best_models, overall_best):\n        \"\"\"Generate comprehensive final report\"\"\"\n        report_path = get_report_path('ml_baseline_models_report.md')\n        \n        with open(report_path, 'w') as f:\n            f.write(\"# √úbung 5 - Baseline Models Report\\n\\n\")\n            f.write(f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n            \n            f.write(\"## Executive Summary\\n\\n\")\n            best_name = overall_best[0]\n            best_variant = overall_best[1]['variant_name']\n            best_strategy = overall_best[1]['strategy']\n            best_val_auc = overall_best[1]['val_auc']\n            best_test_auc = overall_best[1]['test_auc']\n            \n            f.write(f\"- **Best Model:** {best_name} with {best_strategy} strategy\\n\")\n            f.write(f\"- **Validation AUC:** {best_val_auc:.3f}\\n\")\n            f.write(f\"- **Test AUC:** {best_test_auc:.3f}\\n\")\n            f.write(f\"- **Class Imbalance Ratio:** 13.2:1 (handled with {best_strategy})\\n\\n\")\n            \n            f.write(\"## Model Performance Summary\\n\\n\")\n            for model_name, model_info in best_models.items():\n                variant_name = model_info['variant_name']\n                test_metrics = self.results[model_name][variant_name]['test_metrics']\n                \n                f.write(f\"### {model_name.replace('_', ' ').title()} ({model_info['strategy']})\\n\")\n                f.write(f\"- **ROC-AUC:** {test_metrics['roc_auc']:.3f}\\n\")\n                f.write(f\"- **PR-AUC:** {test_metrics['pr_auc']:.3f}\\n\")\n                f.write(f\"- **F1-Score:** {test_metrics['f1']:.3f}\\n\")\n                f.write(f\"- **Precision:** {test_metrics['precision']:.3f}\\n\")\n                f.write(f\"- **Recall:** {test_metrics['recall']:.3f}\\n\\n\")\n                \n            f.write(\"## Class Imbalance Strategies Tested\\n\\n\")\n            f.write(\"1. **No Balancing:** Original imbalanced dataset\\n\")\n            f.write(\"2. **Class Weights:** Balanced class weights in model\\n\")\n            f.write(\"3. **SMOTE:** Synthetic Minority Oversampling Technique\\n\")\n            f.write(\"4. **SMOTE + Tomek:** Hybrid oversampling + undersampling\\n\\n\")\n            \n            f.write(\"## Key Findings\\n\\n\")\n            f.write(\"- Class imbalance handling significantly improved minority class detection\\n\")\n            f.write(\"- SMOTE-based approaches showed best performance for this dataset\\n\")\n            f.write(\"- Random Forest generally outperformed Logistic Regression\\n\")\n            f.write(\"- Model explanations via SHAP reveal most important predictive features\\n\\n\")\n            \n            f.write(\"## Visualizations Generated\\n\\n\")\n            f.write(\"1. **ROC Curves:** `docs/visualizations/models/roc_curves_comparison.png`\\n\")\n            f.write(\"2. **PR Curves:** `docs/visualizations/models/pr_curves_comparison.png`\\n\")\n            f.write(\"3. **Metrics Heatmap:** `docs/visualizations/models/metrics_heatmap.png`\\n\")\n            f.write(\"4. **SHAP Feature Importance:** `docs/visualizations/models/shap_feature_importance.png`\\n\")\n            f.write(\"5. **SHAP Summary Plot:** `docs/visualizations/models/shap_summary_plot.png`\\n\\n\")\n            \n            f.write(\"## Next Steps\\n\\n\")\n            f.write(\"1. **Hyperparameter Tuning:** Optimize model parameters\\n\")\n            f.write(\"2. **Advanced Models:** Implement ensemble methods\\n\")\n            f.write(\"3. **Feature Selection:** Remove redundant features\\n\")\n            f.write(\"4. **Temporal Validation:** Cross-validation with temporal splits\\n\")\n            f.write(\"5. **Clinical Validation:** Collaborate with medical experts\\n\")\n            \n        self.logger.info(f\"üìÑ Final report saved: {report_path}\")\n\ndef main():\n    \"\"\"Main execution function\"\"\"\n    print(\"üöÄ Starting Step 5: Baseline Model Implementation\")\n    print(\"=\" * 60)\n    \n    try:\n        # Initialize trainer\n        trainer = BaselineModelTrainer()\n        \n        # Load processed data\n        X_train, X_val, X_test, y_train, y_val, y_test = trainer.load_processed_data()\n        \n        # Handle class imbalance\n        strategies = trainer.handle_class_imbalance(X_train, y_train)\n        \n        # Train models\n        models = trainer.train_baseline_models(strategies, X_train, X_val, y_train, y_val)\n        \n        # Evaluate models\n        results = trainer.evaluate_models(models, X_val, y_val, X_test, y_test)\n        \n        # Select best models\n        best_models, overall_best = trainer.select_best_models()\n        \n        # Create visualizations\n        trainer.create_evaluation_visualizations(best_models)\n        \n        # Generate explanations\n        trainer.explain_best_model(best_models, overall_best)\n        \n        # Save models\n        trainer.save_models(best_models)\n        \n        # Generate final report\n        trainer.generate_final_report(best_models, overall_best)\n        \n        print(\"\\n‚úÖ Step 5 completed successfully!\")\n        print(f\"üèÜ Best model: {overall_best[0]} ({overall_best[1]['variant_name']})\")\n        print(f\"üìä Test AUC: {overall_best[1]['test_auc']:.3f}\")\n        print(\"üíæ Models saved to: models/\")\n        print(\"üìä Visualizations saved to: docs/visualizations/models/\")\n        print(\"üìã Check docs/reports/ml_baseline_models_report.md for detailed results\")\n        print(\"\\nüéâ √úbung 5 ML Implementation Complete!\")\n        \n        return 0\n        \n    except Exception as e:\n        print(f\"‚ùå Baseline modeling failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        return 1\n\nif __name__ == \"__main__\":\n    exit(main())
